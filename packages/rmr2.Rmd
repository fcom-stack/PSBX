---
title: "PACKAGE RMR2"
output:
  pdf_document: default
  html_document: default
---

```{r eval=FALSE, include = FALSE}
tinytex::install_tinytex()
```


**RAPPEL**


**I- INTRODUCTION AU PACKAGE RMR2**

- a. Pré-requis et installation
- b. Fonctions utiles
  
**II- APPLICATIONS ELEMENTAIRES**

- a. Ab Initio
- b. Comptage d’entiers
- c. Comptage de mots
  
**III- APPLICATIONS AVANCEES**

- a. Transmission d'un dataframe
- b. Régression linéaire

  
**CONCLUSION**


**RESSOURCES**

\newpage

# RAPPEL

Avant de commencer, voici un bref rappel de ce que c'est que le **mapreduce**.

MapReduce est un modèle de programmation créé par Google pour le traitement et la génération de larges ensembles de données sur des clusters d’ordinateurs. Il s’agit d’un composant central du Framework logiciel Apache Hadoop, qui permet le traitement résilient et distribué d’ensembles de données non structurées massifs sur des clusters d’ordinateurs, au sein desquels chaque nœud possède son propre espace de stockage. 

Chaque algorithme de traitement de données basé sur MapReduce doit contenir deux types de programmes:

- Les Mappers : Ce sont des fonctions qui reçoivent en entrée les données à traiter et produisent pour chaque ligne de données, un résultat sous forme d'une paire (clé,valeur).
- Les Reducers : Ces fonctions reçoivent les résultats des mappers et les réduisent au moyen de diverses sortes d'opérations afin d'en obtenir un résultat plus concis correspondant à celui attendu par l'utilisateur.


\newpage

# I- INTRODUCTION AU PACKAGE RMR2

Dans ce tutoriel, nous nous intéressons à la programmation de MapReduce en R. Nous utiliserons la technologie RHadoop de la société Revolution Analytics et en particulier le package "rmr2" qui permet d'apprendre la programmation de MapReduce sans avoir à installer l'environnement Hadoop. Le paquet rmr2 permet d'effectuer de gros traitements et analyses de données via MapReduce sur un cluster Hadoop.

Après avoir installer le package rmr2, nous présenterons dans un premier temps des exemples très simples de cas d'utilisations, puis, dans un deuxième temps, nous progresserons en programmant un algorithme simple d'exploration de données comme la régression linéaire multiple.



## a- Pré-requis et installation

La librairie rmr2 n’est pas disponible sur le CRAN, il faut donc l'installer depuis github. Et pour cela nous allons nous servir du package devtools. Ce package vous permet d'installer des packages dans R-studio directement depuis github

- Tout d'abord, installez devtools

```{r eval=FALSE}
install.packages("devtools")
library(devtools)
```


- Ensuite installez rmr2

```{r eval=FALSE}
devtools::install_github(c('RevolutionAnalytics/rmr2/pkg'))
```



## b- Fonctions utiles

Il conviendra d’utiliser l’aide en ligne pour avoir une description détaillée des différentes fonctions du package rmr2. Voici quelques fonctions qui seront utiles pour la suite.

- Lire ou écrire des objets "R" depuis ou vers le système de fichiers:

from.dfs, to.dfs

- Créer, projeter ou concaténer des paires clé-valeur:

keyval

- MapReduce en utilisant le streaming Hadoop:

mapreduce

- L'objet "big-data":

big.data.object

- Manipulation de fichiers: 

dfs.empty

- Equijoins utilisant map-reduce: 

equijoin

- Paramètres importants de Hadoop en relation avec la rmr2: 

hadoop.settings

- Créer des combinaisons de paramètres pour des OI flexibles:

make.input.format

- Fonction permettant de définir et d'obtenir les options du paquet:

rmr.options

- Exemples de grands ensembles de données:

rmr.sample

- Afficher le contenu d'une variable:

rmr.str

- Fonctions permettant de diviser un fichier en plusieurs parties ou de fusionner plusieurs parties en une seule:

scatter

- Définir le statut et définir et incrémenter les compteurs pour un emploi Hadoop:

status

- Creer des fonctions map-reduce à partir d'autres fonctions:

to.map

\newpage

# II- APPLICATIONS ELEMENTAIRES


## a- Ab Initio

On commence par charger la librarie rmr2 et spécifier l’utilisation en locale,c'est à dire sans serveur Hadoop. La deuxième commande nous donne la possibilité de pratiquer la programmation de MapReduce sans avoir à installer l'environnement Hadoop.

```{r}
library(rmr2) 
rmr.options(backend = "local")
```

Exécuter les instructions suivantes en prenant soin de bien identifier les types d’objets manipulés.

- Lire ou écrire des objets "R" depuis ou vers le système de fichiers

```{r}
# création d'un objet de type big data
test <- to.dfs(1:20)

# retour à R
test2 <- from.dfs(test)
test2
```

La première ligne place les données dans le HDFS, où la majeure partie des données doit résider pour que mapreduce puisse fonctionner. Il n'est pas possible d'écrire du big data de manière évolutive avec "to.dfs". "to.dfs" est néanmoins très utile pour diverses utilisations comme l'écriture de cas de test, l'apprentissage et le débogage. "to.dfs" peut mettre les données dans un fichier de votre choix, mais si vous n'en spécifiez pas un, il créera des fichiers temporaires et les nettoiera une fois terminé. La valeur de retour est ce que nous appelons un objet big data. Vous pouvez l'assigner à des variables, la passer à d'autres fonctions rmr, cartographier des emplois ou la relire. C'est un talon, c'est-à-dire que les données ne sont pas en mémoire, mais seulement des informations qui aident à trouver et à gérer les données. De cette façon, vous pouvez vous référer à de très grands ensembles de données dont la taille dépasse les limites de la mémoire.

Il faut bien comprendre que la fonction to.dfs n’est utile que pour une utilisation locale de rmr2. Pour une utilisation réelle, on utilisera des fichier hdfs préexistants.

"from.dfs" est complémentaire de "to.dfs" et renvoie une collection de paires de clés-valeurs. "from.dfs" est utile pour définir des algorithmes de mapreduce chaque fois qu'un mapreduce produit quelque chose de taille raisonnable, comme un résumé, qui peut tenir en mémoire et doit être inspecté pour décider des prochaines étapes, ou pour le visualiser. Il est beaucoup plus important que le "to.dfs" dans le travail de production.


- Créer des paires clé-valeur

```{r}
# création d'une liste de (clef,valeur)
test3 <- keyval(1,1:20)
keys(test3)
values(test3)
```

La fonction keyval() renvoie les paires clé-valeur, c'est-à-dire qu'elle associe une clé à chaque valeur du vecteur d'entrée.


- Mapping clé valeur avec le dataset mtcars

```{r}
# mtcars est un data frame contenant la variable
# nombre de cylindres. Cette variables est définie comme clé
# la valeur associée est la ligne correspondante du data frame 

keyval(mtcars[,"cyl"],mtcars)
```

Dans cet exemple, nous avons pris comme clés les valeurs présentes dans la colonne "cyl".


- Utilisation du mapreduce 

La fonction mapreduce() est essentielle. Il faut ici trois paramètres :
**"input" est l'ensemble de données à traiter ;
**"map" est la fonction appelée pour cartographier les données en paires clé-valeur ;
**"reduce" traite le sous-ensemble de données et renvoie le résultat avec sa clé.

Voici un exemple d’utilisation de la fonction mapreduce. Il consiste à calculer des carrées d’entiers.

```{r}
# carrés d'entiers
entiers <- to.dfs(1:10) 
calcul.map = function(k,v){
  keyval(v,v^2) 
}

# la fonction reduce est nulle par defaut
calcul <- mapreduce(input = entiers,map = calcul.map)

resultat <- from.dfs(calcul) 
resultat
```

Le deuxième exemple consiste à calculer la somme de carrés d’entiers.

```{r}
calcul2.map = function(k,v){ keyval(1,v^2)}
calcul2.reduce = function(k,v){sum(v) }

calcul2 <- mapreduce( input = entiers, map = calcul2.map, reduce = calcul2.reduce)

resultat2 <- from.dfs(calcul2) 
resultat2                      
```


## b- Comptage d’entiers

Il s’agit de compter les nombres d’occurence de 50 tirages d’une loi de Bernoulli de paramètres 32 et 0,4. La fonction tapply le réalise en une seule ligne mais c’est encore un exemple didactique illustrant l’utilisation du paradigme mapreduce.

```{r}
tirage <- to.dfs(rbinom(32,n=50,prob=0.4))
# le map associe à chaque entier une paire (entier,1)

comptage.map = function(k,v){
keyval(v,1) }

comptage.reduce = function(k,v){ keyval(k,length(v))
}

comptage <- mapreduce(
input = tirage,
map = comptage.map, reduce = comptage.reduce)

from.dfs(comptage) 
table(values(from.dfs(tirage)))
```


## c- Comptage de mots

Il est temps maintenant de présenter l’exemple canonique (hello world) de MapReduce qui consiste à compter les mots d’un texte. Le principe est le même que pour le comptage d’entiers, à la différence près que l’étape map requiert plus de travail préparatoire pour découper le texte en mots.


```{r}
#On définit une fonction wordcount pour le comptage de mots
wordcount = function(input,pattern = " "){
#input : texte à analyser au format big data
#pattern : sigle utilisé pour la séparation des mots 
# (" " par défaut)
  wordcount.map = function(k,texte){
    keyval(unlist(strsplit(x = texte, split = pattern)),1) 
  }
  wordcount.reduce = function(word,count){ 
    keyval(word, sum(count))
  } 
  resultat<-mapreduce(
  input = input,
  map = wordcount.map, reduce = wordcount.reduce)
  
  return(resultat)
}

#Un exemple d'utilisation avec un texte simple
texte = c("un petit texte pour l'homme mais un grand grand grand texte pour l'humanité'") 
from.dfs(wordcount(to.dfs(texte)))
```




# III- APPLICATIONS AVANCEES


## a- Transmission d'un dataframe

Dans cette section, nous voulons calculer la somme des carrés des résidus (sum of the squared residuals - SSR) pour une analyse unidirectionnelle de la variance (ANOVA). L'originalité ici réside dans la manipulation et la transmission d'une data frame aux nœuds. Il s'agira de subdiviser la data frame en plusieurs parties.

Préparation des données: Nous créons notre jeu de données comme suit :

```{r}
#group membership of the individuals
y <- factor(c(1,1,2,1,2,3,1,2,3,2,1,1,2,3,3))
#values of the response variable
x <- c(0.2,0.65,0.8,0.7,0.85,0.78,1.6,0.7,1.2,1.1,0.4,0.7,0.6,1.7,0.15)
#create a data frame from y and x
don <- data.frame(cbind(y,x))
don
```
Ci dessus notre tableau de données (format interne data.frame)

Nous devons définir les procédures de map() et de reduce().

MAP: Y est une variable catégorielle, elle indique l'appartenance au groupe. Nous l'utilisons directement pour définir les éléments clés.

```{r}
#map
map_ssq <- function(., v){
  #the column y is the key
  cle <- v$y
  #return key and the entire data frame
  return(keyval(cle,v)) 
}
```

La fonction renvoie l'objet dataframe avec la clé en utilisant la fonction keyval().

REDUCE: Le dataframe initial est subdivisé en sous-ensembles définis par Y. Nous calculons la somme des carrés à l'intérieur de chaque groupe. "v" est une partie du cadre de données "don" ici. Nous avons toutes les variables mais seulement une partie des lignes.

```{r}
#reduce
reduce_ssq <- function(k,v){
  #print the subset of the data frame used 
  print("reduce") ; print(v)
  #number of row of the data frame
  n <- nrow(v)
  # calculate the sum of squares
  ssq <- (n-1) * var(v$x)
  #return the key and the result (value) 
  return(keyval(k,ssq))
}
```

"v" est un objet dataframe, nous utilisons nrow() et non length() pour obtenir le nombre de lignes. Nous utilisons aussi l'opérateur "$" pour lire la colonne "x". La fonction renvoie en sortie la clé et le résultat du calcul.

CALCUL: Nous appelons la procédure mapreduce() de "rmr2".

```{r}
#rmr2 format
don.dfs <- to.dfs(don)

#mapreduce
calcul <- mapreduce(input=don.dfs,map=map_ssq,reduce=reduce_ssq)

#retrieve the result
resultat <- from.dfs(calcul) 
print(resultat)
```

Nous obtenons les somme des carrés des éarts à l'intérieur de chaque sous-population qui sont identifiées par l'élément clé.
Nous effectuons ensuite la somme pour obtenir la somme des carrés des résidus.

```{r}

#SSR
ssr <- sum(resultat$val) 
print(ssr)
```

Nous obtenons la valeur SSR = 2.587758.

C’est le bon résultat que l’on peut obtenir avec la fonction AOV de R par exemple.

```{r}
#contrôle
print(aov (x ~ y))
```



## b- Régression linéaire

Dans cette section. Les données sont découpées en 2 blocs par la fonction map() (avec 2 valeurs de clés distinctes – la généralisation à K blocs ne pose pas de problèmes, il suffit de modifier la fonction map). 

Les calculs sont réalisés pour chaque bloc par la fonction reduce(). A la sortie, nous effectuons la consolidation en additionnant les matrices. 

Nous profiterons également de cet exemple pour aller plus loin dans la manipulation des données. Plutôt que de renvoyer une valeur atomique à la sortie de la fonction reduce(), nous enverrons une structure un peu plus complexe. Nous pourrons ainsi évaluer la souplesse de l’outil lorsqu’il s’agit d’aller vers des traitements plus élaborés.


**Données: Nous utiliserons les données mtcars [data(mtcars)]. Nous cherchons à expliquer la consommation (mpg) en fonction des autres variables.

```{r}
mtcars
```

**MAP: La stratégie map() consiste à subdiviser les données (le data frame) en plusieurs parties. Voici le code pour une partition aléatoire en 2 portions à peu près égales.

```{r}
#map
map_lm <- function(., D){
  #génération de valeurs aléatoires
  alea <- runif(nrow(D))
  #clé - découpage en 2 parts à peu près égales
  #on peut facilement multiplier les sous-groupes 
  cle <- ifelse(alea < 0.5, 1, 2)
  #renvoyer la clé et les données
  return(keyval(cle,D))
}
```

Remarque 1 : Le caractère aléatoire de la partition n’est pas obligatoire dans le contexte de la régression. Nous aurions pu tout aussi bien prendre les n1 premiers individus pour la 1ère portion et les n2 suivants pour la seconde (avec taille d’échantillon = n = n1 + n2). Par conséquent, si les fragments de données sont situés sur des machines différentes, il sera tout à fait possible d’effectuer les calculs localement avant de consolider les résultats.

Remarque 2 : La généralisation en une subdivision en K sous-groupes d’observations ne pose absolument aucun problème. Ainsi, le code reduce() et la consolidation qui suivent fonctionneront quel que soit le nombre de nœuds sollicités.

**REDUCE: Penchons-nous un peu sur l’estimation des moindres carrés ordinaires (MCO) avant de décrire la fonction reduce(). Le modèle s’écrit :

![](picture/modele.png)

Y est la variable cible ; X est la matrice correspondant aux variables prédictives, une première colonne de valeurs 1 est accolée à la matrice pour tenir compte de la constante de la régression ; a est le vecteur des paramètres ; "e" est le terme d’erreur qui résume les insuffisances du modèle.

L’estimateur des moindres carrés ordinaires â est défini par la formule:

![](picture/EMC.png)

Où Xt est la transposée de la matrice X.

Regardons de près les coefficients des matrices pour comprendre la décomposition des calculs. Pour (XtX), au croisement des variables Xj et Xm, nous avons :

![](picture/croisement1.png)

Les termes étant additifs, nous pouvons fractionner les calculs en 2 parties :

![](picture/fractionner.png)

Il en est de même pour (Xty), au croisement de Xj et y:

![](picture/croisement2.png)

Subdiviser les calculs en K parties ne pose absolument aucun problème au regard de ces propriétés. Nous les exploitons (ces propriétés) pour écrire la fonction reduce() :

```{r}
#reduce
reduce_lm <- function(k,D){
  #nombre de lignes
n <- nrow(D)
  #récupération de la cible
y <- D$mpg
  #prédictives
X <- as.matrix(D[,-1])
  #rajouter la constante en première colonne
X <- cbind(rep(1,n),X)
  #calcul de X'X
XtX <- t(X) %*% X
  #calcul de X'y
Xty <- t(X) %*% y
  #former une structure de liste
res <- list(XtX = XtX, Xty = Xty)
  #renvoyer le tout
  return(keyval(k,res))
}
```


La nouvelle subtilité est que nous utilisons une liste pour renvoyer les deux matrices (XtX) et (Xty). Il faudra être très attentif lorsqu’il faudra consolider les résultats pour former les matrices globales correspondantes.

**Calculs et récupération des résultats: Il ne reste plus qu’à la lancer les calculs...

```{r}
#format rmr2
don.dfs <- to.dfs(mtcars)
#mapreduce
calcul <- mapreduce(input=don.dfs,map=map_lm,reduce=reduce_lm)
#récupération
resultat <- from.dfs(calcul) 
print(resultat)
```

Voyons en détail l’objet « résultat » :

Dans $key, nous disposons du vecteur (1, 1, 2, 2), nous remarquons que les clés se répètent 2 fois parce que notre fonction reduce() a retourné 2 éléments (XtX) et (Xty).

Dans $val, nous avons une structure de liste où les matrices (XtX) et (Xty) se succèdent pour chaque valeur de la clé. Pour former la matrice (XtX) globale [resp. (Xty)], il faudrait additionner les éléments en position (1, 3) [resp. (2, 4)].

**Consolidation des résultats: Les procédures de consolidation suivantes sont opérationnelles quel que soit le nombre de nœuds sollicités (c.-à-.d. nombre de clés K >= 1).

```{r}
#consolidation
#X'X
MXtX <- matrix(0,nrow=ncol(mtcars),ncol=ncol(mtcars)) 
for (i in seq(1,length(resultat$val)-1,2)){
  MXtX <- MXtX + resultat$val[[i]] 
}
print("MXtX")
print(MXtX)

#X'y
MXty <- matrix(0,nrow=ncol(mtcars),ncol=1) 
for (i in seq(2,length(resultat$val),2)){
MXty <- MXty + resultat$val[[i]] 
}
print("MXty")
print(MXty)
```

Nous obtenons les matrices globales (XtX) et (Xty)

**Estimation des paramètres de la régression: Les estimateurs â sont produits à l’aide de procédure solve() de R.

```{r}
#coefficients de la régression
a.chapeau <- solve(MXtX,MXty) 
print("A l’issue des calculs, les coefficients de la régression pour les données « mtcars » sont :")
print(a.chapeau)
```

**Vérification - Procédure lm() de R: A titre de vérification, nous avons effectué la régression à l’aide de la procédure lm() de R.

```{r}
#vérification
print(summary(lm(mpg ~ ., data = mtcars)))
```

Les paramètres estimés concordent en tous points.


\newpage


# CONCLUSION

Des exemples très scolaires ont été mis en avant dans ce tutoriel pour illustrer la programmation MapReduce à l’aide du package « rmr2 » sous R. L’idée directrice est la subdivision des calculs sur un groupe (cluster) de machines (nœuds). Bien sûr, d’autres solutions existent.

Pour aller plus loin, il faudrait se placer sur une configuration où les données arrivent par blocs - par exemple en provenance de différentes machines - occasionnant plusieurs appels à la fonction map qui les réorganise avant de passer la main à la fonction reduce, qui peut être appelée plusieurs fois ou non selon le nombre de valeurs distinctes de la clé. Ceci serait possible par exemple si l'on travaillait dans un véritable environnement Hadoop avec un cluster à plusieurs nœuds.


\newpage


# RESSOURCES
- http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/en_Tanagra_MapReduce.pdf
- https://www.math.u-bordeaux.fr/~arichou/TP.pdf
